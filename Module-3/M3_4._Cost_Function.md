# Logistic regression — cost function (every tiny step, fully expanded)

---

### 1) The piecewise cost (why two cases)

You wrote:

$$
\text{Cost}(h_\theta(x),y)=
\begin{cases}
-\log(h_\theta(x)) & \text{if } y=1,\\[4pt]
-\log(1-h_\theta(x)) & \text{if } y=0.
\end{cases}
$$

**Explanation (step-by-step):**

* We want a penalty that is **small** when the model predicts correctly and **large** when the model predicts wrongly and confidently.
* If true label $y=1$:

  * Good prediction ⇔ $h_\theta(x)$ close to $1$ ⇒ want small penalty. Use $-\log(h_\theta(x))$ because:

    * If $h=1$, $-\log(1)=0$ → zero cost (perfect).
    * If $h$ is small (near 0), $\log(h)$ is large negative → $-\log(h)$ is large positive → big penalty.
* If true label $y=0$:

  * Good prediction ⇔ $h_\theta(x)$ close to $0$ ⇒ want small penalty. Use $-\log(1-h_\theta(x))$ because:

    * If $h=0$, $-\log(1-0)=-\log(1)=0$ → zero cost.
    * If $h$ is large (near 1), $1-h$ small → $-\log(1-h)$ large → big penalty.

**Memory hook:** pick the log of the probability that corresponds to the true class, negate it.

---

### 2) Unified formula (avoiding if-cases)

We combine the two cases into one formula:

$$
\text{Cost}(h,y) = -y\log(h) - (1-y)\log(1-h).
$$

**Tiny-step check that it equals the piecewise form:**

* If $y=1$: right side = $-1\cdot\log(h) - 0\cdot\log(1-h) = -\log(h)$ ✅.
* If $y=0$: right side = $-0\cdot\log(h) - 1\cdot\log(1-h) = -\log(1-h)$ ✅.

Why this is useful: programmers can implement one formula instead of branching (`if y==1 else ...`), and it generalizes smoothly to vectorized code.

---

### 3) What about `-log(0)`? (careful limit and numerical meaning)

* The natural log, $\log(x)$, blows up to $-\infty$ as $x\to 0^+$. Symbolically:

  $$
  \lim_{p\to 0^+} \log(p) = -\infty.
  $$
* So $-\log(0)$ is treated as $+\infty$ (penalty infinitely large). In practice:

  * If model predicts $h=0$ but true $y=1$ → infinite penalty (extreme mistake).
  * If model predicts $h=1$ but true $y=0$ → $-\log(1-1) = -\log(0) = +\infty$.

**Intuition:** being 100% sure and wrong is catastrophically bad according to cross-entropy.

---

### 4) Numeric examples — how cost behaves for different $h$ and $y$

Take sample predictions $h$ and compute cost for three target cases $y=1$, $y=0$, $y=0.5$. (Remember: normally $y$ is 0 or 1; $y=0.5$ is a soft target used sometimes — explained later.)

Below are exact numeric costs (natural log) for common $h$ values.

|  $h$ | Cost when $y=1$ = $-\log(h)$ | Cost when $y=0$ = $-\log(1-h)$ | Cost when $y=0.5$ = $-0.5\log(h)-0.5\log(1-h)$ |
| ---: | ---------------------------: | -----------------------------: | ---------------------------------------------: |
|  1.0 |                          0.0 |                      $+\infty$ |                                      $+\infty$ |
| 0.99 |          0.01005033585350145 |              4.605170185988091 |                              2.307610260920796 |
|  0.9 |          0.10536051565782628 |              2.302585092994046 |                             1.2039728043259361 |
|  0.8 |           0.2231435513142097 |             1.6094379124341005 |                             0.9162907318741551 |
|  0.5 |           0.6931471805599453 |             0.6931471805599453 |         0.6931471805599453 (minimum for y=0.5) |
|  0.2 |           1.6094379124341003 |             0.2231435513142097 |                              0.916290731874155 |
|  0.1 |           2.3025850929940455 |            0.10536051565782628 |                              1.203972804325936 |
| 0.01 |            4.605170185988091 |            0.01005033585350145 |                              2.307610260920796 |
| 1e-6 |           13.815510557964274 |        0.000001000000500029089 |                              6.907755778982387 |
|  0.0 |                    $+\infty$ |                            0.0 |                                      $+\infty$ |

**Step-by-step reading of one cell (example):**

* For $h=0.8, y=1$: cost $= -\log(0.8)$. Compute $\log(0.8) \approx -0.2231435513142097$. Negate → cost $= 0.2231435513142097$. That is a small penalty because prediction is fairly confident and correct.
* For $h=0.8, y=0$: cost $= -\log(1-0.8) = -\log(0.2)$. Compute $\log(0.2)\approx -1.6094379124341003$. Negate → cost $= 1.6094379124341003$ (large penalty because model was confidently wrong).

**Observation:** cost grows rapidly as the model becomes confident and wrong (closer to 0 or 1 but matching the wrong class).

---

### 5) What if $y=0.5$? (why user asked + what it means)

* Normally $y\in\{0,1\}$. But you can plug $y=0.5$ into the unified formula. This represents a **soft/uncertain target** (not a standard hard label).
* Formula becomes:

  $$
  \text{Cost}(h,0.5) = -0.5\log(h) - 0.5\log(1-h) = -\tfrac{1}{2}\log\big(h(1-h)\big).
  $$
* $h(1-h)$ is maximized when $h=0.5$ (value $0.25$), so $\log(h(1-h))$ is largest at $h=0.5$, thus the negative of that is **minimal** at $h=0.5$. Numerically the minimal cost for $y=0.5$ is:

  $$
  -\tfrac{1}{2}\log(0.25) = -\tfrac{1}{2}(-1.3862943611) = 0.69314718056.
  $$
* Interpretation: if the "true" label is uncertain (0.5), the model's best output is $h=0.5$ (be uncertain too), and the minimal cost is \~0.693.

**Use cases for $y$ not being exactly 0 or 1:** label smoothing, teacher-student distillation, soft labels from ensemble — but those are extra implementation concepts (kept below as optional).

---

### 6) Gradient of the cost — full tiny-step derivation (why update ends up $(h-y)x$)

We derive derivative for a single training example $(x,y)$. Use notation:

* $$ z = \theta^T x $$ (linear score),
* $$ h = h_\theta(x) = \sigma(z) = \dfrac{1}{1+e^{-z}} $$.

Cost for one example:

$$
J(\theta) = -y\log(h) - (1-y)\log(1-h).
$$

We want $$ \dfrac{\partial J}{\partial \theta_j} $$. Chain rule with intermediate $z$:

**Step A — derivative of cost w\.r.t $z$**

1. Compute $$ \dfrac{dJ}{dz} $$ by chain rule:

   $$
   \frac{dJ}{dz} = -y \frac{d}{dz}[\log(h)] - (1-y)\frac{d}{dz}[\log(1-h)].
   $$

2. Use derivative of $$ \log(h) $$: $$ \dfrac{d}{dz}\log(h) = \dfrac{1}{h}\cdot \dfrac{dh}{dz} $$. Also $$ \dfrac{d}{dz}\log(1-h) = \dfrac{1}{1-h}\cdot \dfrac{d(1-h)}{dz} $$.

   So

   $$
   \frac{dJ}{dz} = -y\frac{1}{h}\frac{dh}{dz} - (1-y)\frac{1}{1-h}\frac{d(1-h)}{dz}.
   $$

3. Note $$ \dfrac{d(1-h)}{dz} = -\dfrac{dh}{dz} $$. Substitute:

   $$
   \frac{dJ}{dz} = -y\frac{1}{h}\frac{dh}{dz} - (1-y)\frac{1}{1-h}\big(-\frac{dh}{dz}\big).
   $$

4. Factor out $$ \dfrac{dh}{dz} $$:

   $$
   \frac{dJ}{dz} = -\frac{dh}{dz}\Big( \frac{y}{h} - \frac{1-y}{1-h} \Big).
   $$

5. Combine the fraction inside parentheses to a single fraction:

   $$
   \frac{y}{h} - \frac{1-y}{1-h} = \frac{y(1-h) - (1-y)h}{h(1-h)}.
   $$

   Expand numerator:

   $$
   y(1-h) - (1-y)h = y - yh - h + yh = y - h.
   $$

   So inside parentheses becomes:

   $$
   \frac{y-h}{h(1-h)}.
   $$

6. Therefore:

   $$
   \frac{dJ}{dz} = -\frac{dh}{dz}\cdot \frac{y-h}{h(1-h)}.
   $$

7. Use derivative of sigmoid: $$ \dfrac{dh}{dz} = h(1-h) $$. Substitute:

   $$
   \frac{dJ}{dz} = -h(1-h)\cdot \frac{y-h}{h(1-h)} = -(y-h) = h - y.
   $$

   **Key cancellation:** $h(1-h)$ cancels numerator/denominator exactly — that is why the expression simplifies so cleanly.

**Result:** $$ \dfrac{dJ}{dz} = h - y $$.

**Step B — derivative w\.r.t parameter $$ \theta_j $$**

8. $$ z = \sum_k \theta_k x_k $$, so $$ \dfrac{\partial z}{\partial \theta_j} = x_j $$.

9. By chain rule:

   $$
   \frac{\partial J}{\partial \theta_j} = \frac{dJ}{dz} \cdot \frac{\partial z}{\partial \theta_j} = (h-y)\cdot x_j.
   $$

Vector form:

$$
\nabla_\theta J = (h-y)\, x
$$

and for the bias term $$ \theta_0 $$ (where $$ x_0\equiv 1 $$), the derivative is $h-y$.

**Interpretation:** the error term is $(h-y)$ (predicted minus actual). Multiply by input feature to get how much to change that weight.

---

### 7) Why gradient descent update *looks the same* as in linear regression

* In linear regression with squared error (single example), cost $$ J_{\text{lin}} = \tfrac{1}{2}(h-y)^2 $$ where $$ h=\theta^T x $$. Compute derivative:

  $$
  \frac{\partial J_{\text{lin}}}{\partial \theta_j} = (h-y)\cdot \frac{\partial (h)}{\partial \theta_j} = (h-y)\cdot x_j.
  $$
* So both linear regression (MSE) and logistic regression (cross-entropy with sigmoid) produce the same **algebraic form** for the gradient: $(h-y)x$.
* Therefore a gradient descent update step for a single example:

  $$
  \theta \leftarrow \theta - \eta (h-y)x
  $$

  has the *same-looking* formula in both models.

**But important distinction (do not skip):**

* Although the update formula is the same form, the two models are different because:

  * In linear regression $$ h=\theta^T x $$ (linear), while in logistic $$ h=\sigma(\theta^T x) $$ (sigmoid of a linear score). So the numerical value of $h$ differs.
  * The loss surfaces are different (MSE is a quadratic in parameters for linear models; cross-entropy with sigmoid is convex in $$ \theta $$ but shaped differently). The convergence behavior, step-size sensitivity, and model interpretation differ.
* The neat simplification in logistic regression is thanks to the exact cancellation of the sigmoid derivative with denominators that appear when differentiating the log terms (the algebraic reason we saw in step 6).

---

### 8) Practical note about $-\log(0)$ and implementation (optional but important)

* In real code you must **avoid passing exact 0 or 1** into $\log$. Two common solutions:

  1. **Clipping outputs**: replace $h$ with $$ \mathrm{clip}(h, \varepsilon, 1-\varepsilon) $$ where $$ \varepsilon $$ is small (e.g., $10^{-15}$ or $10^{-12}$). Then compute $-\log(\cdot)$.
  2. **Use numerically stable expressions**: compute log-sum-exp forms or use functions like `logsigmoid` and `binary_cross_entropy_with_logits` (which accept the pre-sigmoid `z` and compute loss in a numerically stable way).
* Reason: floating-point `log(0)` raises errors or returns `-inf`, and `inf` times anything can break training.

---

### 9) Quick checklist you can memorize

1. Cost per example: $ -y\log(h) - (1-y)\log(1-h)$.
2. If the model is extremely wrong and confident → cost huge (→ training pushes parameters strongly to fix it).
3. Derivative w\.r.t parameters simplifies to $(h-y)x$ because sigmoid derivative cancels nicely with log derivatives.
4. Update step looks like linear regression update, but $h$ is different (sigmoid vs linear), so models behave differently.

---

If you want, next I can:

* produce a tiny worked numeric training step showing $\theta$ values updated with a concrete $x$, $y$ and learning rate (digit-by-digit), **or**
* show a small Python snippet of a numerically-stable logistic loss and gradient (with clipping / `logsumexp`) — tell me which and I’ll write it out fully.
