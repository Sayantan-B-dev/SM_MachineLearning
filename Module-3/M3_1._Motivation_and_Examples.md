# **Logistic Regression — Theoretical Definitions (detailed, step by step, no skipping any small part):**

---

### 1. **Definition of Logistic Regression**

* Logistic regression is a **statistical model** used for **classification tasks**.
* Unlike linear regression, which predicts a continuous value, logistic regression predicts the **probability** that an input belongs to a particular class.
* In the binary case, the output is restricted to two values: $y \in \{0,1\}$.
* The model uses the **sigmoid (logistic) function** to map a linear combination of input features into the range $[0,1]$.

Mathematically:

$$
P(y=1 \mid x) = \sigma(z) = \frac{1}{1+e^{-(w^T x + b)}}
$$

where:

* $x$ = input feature vector,
* $w$ = weight vector,
* $b$ = bias (intercept),
* $z = w^T x + b$,
* $\sigma(z)$ = sigmoid function that “squashes” real numbers into $[0,1]$.

---

### 2. **Definition of the Sigmoid Function**

* The sigmoid function (also called logistic function) is:

$$
\sigma(z) = \frac{1}{1+e^{-z}}
$$

* Domain: all real numbers ($-\infty < z < \infty$).
* Range: $0 < \sigma(z) < 1$.
* At $z=0$, $\sigma(0) = 0.5$.
* As $z \to +\infty$, $\sigma(z) \to 1$.
* As $z \to -\infty$, $\sigma(z) \to 0$.

**Interpretation:** It gives the **probability** of the positive class (label 1).

---

### 3. **Definition of Odds and Log-Odds**

* **Probability** of an event: $p = P(y=1\mid x)$.
* **Odds**: ratio of success to failure:

$$
\text{Odds} = \frac{p}{1-p}.
$$

* **Log-odds (Logit function):**

$$
\text{logit}(p) = \log \left(\frac{p}{1-p}\right).
$$

* Logistic regression assumes the log-odds of the probability are a **linear function** of the input:

$$
\log \left(\frac{p}{1-p}\right) = w^T x + b.
$$

---

### 4. **Definition of Decision Boundary**

* A **decision boundary** is the dividing surface in the feature space that separates classes.
* For binary logistic regression, the boundary is given by:

$$
w^T x + b = 0.
$$

* On one side of this hyperplane, $P(y=1|x) > 0.5$, and on the other side, $P(y=1|x) < 0.5$.

---

### 5. **Definition of Classification Problems**

* **Binary classification:** Problems where the output can take only two values, e.g., spam/not spam, cancerous/non-cancerous.

$$
y \in \{0,1\}.
$$

* **Multiclass classification:** Problems where the output can take more than two values, e.g., apple/banana/orange.

$$
y \in \{0,1,2,\dots,K-1\}.
$$

---

### 6. **Definition of Softmax Function (Multiclass Logistic Regression)**

* In multiclass settings, probabilities must be assigned to **all classes** such that they sum to 1.
* The **softmax function** does this:

$$
p_k = \frac{e^{z_k}}{\sum_{j=1}^K e^{z_j}}
$$

where:

* $z_k = w_k^T x + b_k$ is the score for class $k$,
* $K$ is total number of classes,
* $p_k$ is the probability that input belongs to class $k$.

---

### 7. **Definition of Loss Function (Binary Cross-Entropy / Log Loss)**

* Logistic regression uses **maximum likelihood estimation** for training.
* The loss function (to minimize) is the **negative log-likelihood**, also called **binary cross-entropy**:

$$
J(w,b) = -\sum_{i=1}^N \big[ y_i \log(p_i) + (1-y_i)\log(1-p_i) \big].
$$

where:

* $N$ = number of training samples,
* $p_i$ = predicted probability for the positive class of the $i$-th example.

---

### 8. **Definition of Gradient Descent in Logistic Regression**

* Gradient descent is an optimization algorithm used to minimize the loss.
* In logistic regression:

$$
w \leftarrow w - \eta \sum_{i=1}^N (p_i - y_i)x_i
$$

$$
b \leftarrow b - \eta \sum_{i=1}^N (p_i - y_i)
$$

where $\eta$ = learning rate.

---

### 9. **Definition of Regularization in Logistic Regression**

* Regularization prevents overfitting by penalizing large coefficients.
* **L2 regularization (Ridge):**

$$
J_{\text{reg}} = J + \frac{\lambda}{2}\|w\|^2
$$

* **L1 regularization (Lasso):**

$$
J_{\text{reg}} = J + \lambda \|w\|_1
$$

where $\lambda$ controls penalty strength.

---

### 10. **Definition of Classification Metrics**

* **Accuracy:** fraction of correctly classified examples.
* **Precision:** fraction of predicted positives that are actually positive.
* **Recall (Sensitivity):** fraction of actual positives that are correctly predicted.
* **F1-score:** harmonic mean of precision and recall.
* **ROC-AUC:** area under the curve showing trade-off between TPR and FPR.

---

# Motivation — why logistic regression exists (intuitively + formally)

* Many real problems require assigning items to discrete categories (classification) rather than predicting a real number. Examples from your notes:

  * emails → spam/not spam, label set $y\in\{0,1\}$
  * manufacturing → ok/not ok, $y\in\{0,1\}$
  * medical imaging → cancerous/non-cancerous, $y\in\{0,1\}$
  * dogs → lab/husky/beagle, $y\in\{0,1,2\}$ (multiclass)
  * fruits → apple/banana/orange, $y\in\{0,1,2\}$ (multiclass)

* A direct linear regression $\hat y = w^T x + b$ can produce values outside $[0,1]$, so it is not suitable when we want an output interpreted as a probability. Logistic regression produces probabilities in $[0,1]$ by passing a linear score through a squashing function (the sigmoid for binary, softmax for multiclass), and it has a clean probabilistic interpretation (maximum likelihood estimation).

# Model formulation — binary case (every tiny step)

1. start with a linear score (logit):

   $$
   z = w^T x + b
   $$

   $x$ is the feature vector, $w$ weight vector, $b$ bias (intercept).

2. convert score to probability using the sigmoid function $\sigma$:

   $$
   \sigma(z) = \frac{1}{1 + e^{-z}}
   $$

   output $p=\sigma(z)$ is interpreted as $P(y=1 \mid x)$.

3. decision rule for classification (binary):

   * choose threshold $t$ (default $t=0.5$). Predict $y=1$ if $p\ge t$, else $y=0$.
   * threshold $0.5$ corresponds to $z=0$ because $\sigma(0)=0.5$. Thus the decision boundary is the hyperplane $w^T x + b = 0$.

# Odds, log-odds, interpretability (step-by-step)

* probability $p = P(y=1|x)$. Odds $=\dfrac{p}{1-p}$.
* log-odds (logit) $=\log\left(\dfrac{p}{1-p}\right)$.
* In logistic regression:

  $$
  \log\left(\frac{p}{1-p}\right) = w^T x + b.
  $$

  So the model is linear in the log-odds.
* Interpretation of a coefficient $w_j$: a one-unit increase in $x_j$ (holding other features fixed) multiplies the odds by $\exp(w_j)$. Example: if $w_j=0.5$, $\exp(0.5)\approx 1.6487$ → odds increase by factor $1.6487$ (≈ $+64.87\%$ increase).

# Training: likelihood, loss, gradient (full derivation)

* For a single example $(x_i,y_i)$, model predicts $p_i=\sigma(w^T x_i + b)$. Likelihood contribution:

  $$
  P(y_i\mid x_i;w,b) = p_i^{y_i}(1-p_i)^{1-y_i}.
  $$
* For $N$ independent examples, likelihood:

  $$
  \mathcal{L}(w,b) = \prod_{i=1}^N p_i^{y_i}(1-p_i)^{1-y_i}.
  $$
* Log-likelihood (easier to optimize):

  $$
  \ell(w,b) = \sum_{i=1}^N \big( y_i\log p_i + (1-y_i)\log(1-p_i)\big).
  $$
* Negative log-likelihood (loss to minimize), also called binary cross-entropy:

  $$
  J(w,b) = -\ell(w,b) = -\sum_{i=1}^N \big( y_i\log p_i + (1-y_i)\log(1-p_i)\big).
  $$
* Gradient (compute derivative w\.r.t $w$): start from $p_i=\sigma(z_i)$ with $z_i=w^T x_i + b$.
  Derivation in tiny steps:

  * $\frac{\partial p_i}{\partial z_i} = p_i(1-p_i)$ (derivative of sigmoid).
  * $\frac{\partial z_i}{\partial w} = x_i$.
  * $\frac{\partial}{\partial w} \big( -y_i\log p_i - (1-y_i)\log(1-p_i) \big)$
    \= $-y_i \frac{1}{p_i}\frac{\partial p_i}{\partial w} -(1-y_i)\frac{1}{1-p_i}\frac{\partial(1-p_i)}{\partial w}$.
  * compute $\frac{\partial(1-p_i)}{\partial w} = -\frac{\partial p_i}{\partial w}$.
  * substituting gives:

    $$
    \frac{\partial J_i}{\partial w} = -(y_i(1-p_i) - (1-y_i)(-p_i))x_i
    = (p_i - y_i) x_i.
    $$
  * So for the full dataset:

    $$
    \nabla_w J = \sum_{i=1}^N (p_i - y_i)x_i.
    $$
  * The derivative w\.r.t bias $b$ is:

    $$
    \frac{\partial J}{\partial b} = \sum_{i=1}^N (p_i - y_i).
    $$
* Gradient descent update (batch):

  $$
  w \leftarrow w - \eta \nabla_w J,\qquad b \leftarrow b - \eta \frac{\partial J}{\partial b},
  $$

  where $\eta$ is the learning rate.

# Numeric worked example (every arithmetic digit shown) — single example SGD

Given single example $x=2.5$, true label $y=1$. Initialize $w=1.2$, $b=-0.7$, learning rate $\eta=0.1$.

1. compute linear score $z$:

   * $w\cdot x = 1.2 \times 2.5$. Compute exactly:

     * $1.2 \times 2.5 = (12 \times 25) / 100 = 300 / 100 = 3.0$.
   * add bias: $z = 3.0 + (-0.7) = 3.0 - 0.7 = 2.3$.
2. compute sigmoid $p=\sigma(z)=\dfrac{1}{1+e^{-z}}$.

   * compute $e^{-z} = e^{-2.3}$. Numerically:

     * $e^{-2.3} \approx 0.10025884372280375$.
   * compute denominator $1 + e^{-2.3} = 1 + 0.10025884372280375 = 1.1002588437228038$.
   * reciprocal: $p = 1 / 1.1002588437228038 \approx 0.9088770389851438$.
3. compute loss for this single example (binary cross-entropy):

   * since $y=1$, loss $= -\log(p) = -\log(0.9088770389851438)$.
   * $\log(0.9088770389851438) \approx -0.09554546459796298$.
   * loss $= -(-0.09554546459796298) = 0.09554546459796298$.
4. compute gradients:

   * error term $p - y = 0.9088770389851438 - 1 = -0.09112296101485617$.
   * gradient w\.r.t $w$: $(p-y) \cdot x = -0.09112296101485617 \times 2.5$.

     * compute: $-0.09112296101485617 \times 2.5 = -0.22780740253714044$.
   * gradient w\.r.t $b$: $p-y = -0.09112296101485617$.
5. gradient descent update with $\eta=0.1$:

   * $w_{\text{new}} = w - \eta \cdot \text{grad}_w = 1.2 - 0.1 \times (-0.22780740253714044)$.

     * compute inside: $0.1 \times (-0.22780740253714044) = -0.022780740253714044$.
     * subtract: $1.2 - (-0.022780740253714044) = 1.2 + 0.022780740253714044 = 1.222780740253714$.
   * $b_{\text{new}} = b - \eta \cdot \text{grad}_b = -0.7 - 0.1 \times (-0.09112296101485617)$.

     * compute inside: $0.1 \times (-0.09112296101485617) = -0.009112296101485617$.
     * subtract: $-0.7 - (-0.009112296101485617) = -0.7 + 0.009112296101485617 = -0.6908877038985144$.

Final updated parameters after one SGD step:

* $w \approx 1.222780740253714$
* $b \approx -0.6908877038985144$

(These numbers match the full-precision computations shown above.)

# Multiclass classification — two main approaches (step-by-step)

1. one-vs-rest (OvR, a.k.a. one-vs-all)

   * For K classes train K independent binary logistic classifiers.
   * Classifier $k$ is trained to distinguish class $k$ (positive) vs all others (negative).
   * At inference compute $p_k=\sigma(w_k^T x + b_k)$ for each $k$, then predict class $\arg\max_k p_k$ (or use calibrated scores).
   * Pros: simple, works well in many settings. Cons: inconsistent probability normalization across classes.

2. multinomial logistic regression (softmax / “multiclass logistic regression”)

   * compute K scores $z_k = w_k^T x + b_k$ for $k=1..K$.
   * softmax probabilities:

     $$
     p_k = \frac{e^{z_k}}{\sum_{j=1}^K e^{z_j}}.
     $$
   * cross-entropy loss for a single example with one-hot label $y$: if true class is $t$,

     $$
     L = -\log p_t.
     $$
   * gradient w\.r.t $w_k$ is $(p_k - y_k)x$ (same style as binary).
   * Example numeric softmax (tiny steps): let $z = [1.0, 2.0, 0.5]$.

     * compute exponentials:

       * $e^{1.0} \approx 2.718281828459045$
       * $e^{2.0} \approx 7.38905609893065$
       * $e^{0.5} \approx 1.6487212707001282$
     * sum of exps: $2.718281828459045 + 7.38905609893065 + 1.6487212707001282 = 11.756059198089822$.
     * probabilities:

       * $p_1 = 2.718281828459045 / 11.756059198089822 \approx 0.23122389762214907$
       * $p_2 = 7.38905609893065 / 11.756059198089822 \approx 0.6285317192117625$
       * $p_3 = 1.6487212707001282 / 11.756059198089822 \approx 0.1402443831660885$
     * these $p_k$ sum to 1 (numerical rounding aside).

# Decision boundary geometry and intuition

* Binary logistic: decision boundary is linear (hyperplane) defined by $w^T x + b = 0$. All points on one side predicted 1, other side 0 (for threshold 0.5).
* Multiclass softmax: boundaries between classes are piecewise linear (since comparing $z_i = z_j$ is linear).
* Non-linearity must come from features: create polynomial features, use kernel methods, or use deep networks (feature extractor + logistic/softmax classifier).

# Practical application mapping (your examples) — features, labels, pitfalls, how to set up

1. emails: spam/not spam

   * features: bag-of-words counts, TF-IDF, presence of links, sender reputation, attachments size, time-of-day.
   * preproc: text tokenization, TF-IDF, optional dimensionality reduction (SVD).
   * label imbalance: spam may be minority/majority depending on dataset → use class weights, resampling, or threshold tuning.
   * cost tradeoff: false negative (spam marked as not spam) vs false positive (ham marked as spam). Usually false positives are more costly; choose threshold accordingly.

2. manufacturing: ok / missing-screw / untightened-screw / wrong-label (multi-class)

   * features: sensor readings (torque, depth), vision-derived measurements (screw circle detected, pixel-based template matches), timestamps.
   * tolerance percentage: define acceptable range for numeric features: e.g., screw depth expected 10 mm ± 5% → \[9.5, 10.5]. If depth < lower bound and screw not detected → missing-screw; if depth detected but torque below tolerance → untightened-screw.
   * approach: either threshold rules for obvious cases (rules + tolerances) and logistic/softmax for ambiguous cases; or treat as multiclass model with labels produced during inspection.
   * cost-sensitive: missing screw might be critical → weight its errors higher in loss (class weights) or use custom loss.

3. medical imaging: cancerous/non-cancerous

   * features: images → typically use CNNs to extract high-level features; logistic regression rarely used alone on raw pixels for complex images.
   * pipelines: CNN base → embedding vector → logistic (sigmoid) output for binary or softmax for multi-class. Logistic used as the final classification head where the CNN learns features.
   * evaluation: strongly rely on sensitivity (recall) for disease detection, but also track specificity; calibrate probabilities for clinical decisions.
   * caution: dataset bias, annotation quality, calibration, domain-shift across hospitals.

4. dogs / fruits (multiclass)

   * encode labels as integers or one-hot vectors for softmax.
   * if classes are many and unbalanced, consider hierarchical classification, data augmentation, or sampling methods.

5. facial recognition — two use-cases and how logistic fits

   * verification (is this person X?) → binary decision. Workflow: extract face embedding (FaceNet/ArcFace), then compare embedding similarity to template; logistic/threshold on similarity score can be used.
   * identification (which person) → multiclass: embeddings → softmax classifier (if closed set). For open-set recognition, combine embedding + nearest neighbor + thresholding.

6. self-driving car (multi-class or multi-label classification)

   * perception tasks: object classification (car/pedestrian/sign), semantic segmentation (per-pixel class), traffic light state classification.
   * detection: typically an object detector outputs bounding boxes + classification head that is often softmax-based.
   * control tasks: many are regression (steering angle) or policy (action classification); logistic/softmax may be used in discrete-action policies.
   * note: large class imbalance (many background vs few pedestrians) → use focal loss, class-weighting, or careful sampling.

# Metrics and evaluation (exact formulas + when to prefer)

* confusion matrix entries for binary: TP, FP, TN, FN.
* accuracy $= \dfrac{TP+TN}{TP+TN+FP+FN}$.
* precision $= \dfrac{TP}{TP+FP}$.
* recall (sensitivity) $= \dfrac{TP}{TP+FN}$.
* F1 score $= 2\cdot\dfrac{\text{precision}\cdot\text{recall}}{\text{precision}+\text{recall}}$.
* ROC curve: plot TPR (recall) vs FPR varying threshold; AUC measures separability.
* Precision-Recall curve: preferred when positive class is rare.
* Calibration: predicted probabilities should match empirical frequencies. Use reliability diagrams and Brier score to check calibration. Platt scaling or isotonic regression can recalibrate probabilities.

# Regularization, feature scaling, and numeric issues (step-by-step reasons)

* Regularization (add to loss):

  * L2: $J_{\text{reg}} = J + \tfrac{\lambda}{2}\|w\|^2$. Gradient adds $\lambda w$.
  * L1: $J_{\text{reg}} = J + \lambda \|w\|_1$. Encourages sparsity, handled with subgradient or coordinate methods.
* Why regularize: prevents overfitting when many features, improves numeric stability.
* Feature scaling: normalize features (zero mean, unit variance) or min-max scale. Reason: gradient descent converges faster if features are on similar scales.
* Categorical variables: one-hot encode or use embeddings for high-cardinality categories.
* Numerical stability in softmax: subtract max from $\{z_j\}$: compute $e^{z_k - m}$ where $m=\max_j z_j$ to avoid overflow.

# Handling class imbalance, threshold tuning, and cost-sensitive learning

* class weights: multiply loss for class $j$ by $w_j$ inversely proportional to class frequency.
* resampling: oversample minority (with care) or undersample majority.
* threshold tuning: choose decision threshold to maximize business metric (F1, weighted accuracy) on validation set.
* custom cost: if false negative is far more expensive than false positive, incorporate that cost into loss or use different thresholds per class.

# Limitations of logistic regression (and how to fix them)

* linear decision boundary in the input feature space — fix by:

  * feature engineering (polynomials, interactions),
  * kernel methods (kernel logistic regression),
  * deep networks (learn non-linear features).
* not ideal for very high-dimensional, highly non-linear raw inputs (images, audio) without a learned feature extractor.
* can be sensitive to multicollinearity among features — consider PCA or regularization.

# Interpretability & explainability (tiny steps)

* coefficients $w_j$: $\exp(w_j)$ → multiplicative change in odds per unit change in $x_j$.

  * Example compute: if $w_j=0.5$, $\exp(0.5) \approx 1.6487212707001282$ → odds multiplied by \~1.6487 (≈ +64.87%).
* use SHAP or LIME for single-prediction explanations for more complex pipelines.

# Implementation checklist (practical step-by-step pipeline)

1. data cleaning (handle missing values, correct labels).
2. feature engineering (numeric features, categorical encodings, interactions).
3. split data (train/val/test or k-fold cross-validation).
4. scale numeric features (standardization or min-max).
5. choose model: binary logistic or softmax for multiclass; consider OvR if K is large and you want simplicity.
6. choose optimizer: batch GD for small data, mini-batch SGD/Adam for large datasets.
7. set regularization ($\lambda$), learning rate $\eta$, and other hyperparameters.
8. monitor metrics (accuracy, precision/recall, ROC-AUC, PR-AUC) on validation set.
9. calibrate probabilities if needed (Platt scaling/isotonic).
10. deploy with monitoring (data drift, threshold re-evaluation).

# Final practical notes tied to your specific examples (explicit, detailed)

* spam classification: use text-specific preprocessing (stopword handling, n-grams), sample for concept drift (spam patterns change), keep threshold adjustable by user preference.
* manufacturing QC with tolerance %: combine deterministic thresholds for trivial cases (if measurement outside extreme tolerance → immediate reject) with logistic/softmax to handle borderline cases and ambiguous sensor fusion. Store tolerance as a feature (distance from nominal) so model can learn patterns.
* facial recognition: logistic rarely used on raw pixels — always use learned embeddings (CNN) + softmax or similarity + threshold.
* self-driving: treat perception as many small classification/regression tasks linked in a pipeline; logistic/softmax often appear in classification heads, but the whole system requires careful dataset balancing, augmentation, and safety-aware thresholds.

# Quick reference formulas (compact)

* sigmoid $\sigma(z)=\dfrac{1}{1+e^{-z}}$
* binary loss for one example $L = -\big(y\log p + (1-y)\log(1-p)\big)$
* gradient (one example) $\nabla_w L = (p-y)x$, $\partial_b L = p-y$
* softmax $p_k=\dfrac{e^{z_k}}{\sum_j e^{z_j}}$
* softmax cross-entropy $L = -\sum_k y_k \log p_k$
* regularized loss (L2): $J = J_{\text{data}} + \tfrac{\lambda}{2}\|w\|^2$

# If you want examples next

* I can produce:

  * (A) a step-by-step worked dataset training run (multiple examples, full gradient descent epochs) with per-epoch numbers;
  * (B) a small Python implementation and walkthrough (with explicit prints of intermediate values);
  * (C) how to convert manufacturing tolerance rules into label generation code for training a classifier.

(Stop here and tell me which concrete follow-up you want if you want one of A/B/C — I will produce that exact expanded, digit-by-digit walkthrough.)
