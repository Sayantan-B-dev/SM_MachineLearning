# **Logistic Regression — Multiclass Classification (One-vs-Rest)**

---

### 1) Binary classification vs multiclass classification

**Binary classification:**

* Output $y \in \{0,1\}$.
* Example: tumor **yes/no**, email **spam/not spam**.
* Logistic regression directly predicts probability:

  $$
  P(y=1 \mid x) = h_\theta(x)
  $$

  and threshold 0.5 (or another) decides class.

**Multiclass classification:**

* Output $y \in \{0,1,2,...,K-1\}$ (K classes).
* Example: fruit **apple/banana/orange** → $K=3$.
* Cannot directly use single sigmoid because sigmoid outputs **0–1 probability for one class only**.
* Idea: reduce multiclass problem into **multiple binary problems** → **One-vs-Rest (OvR)**.

---

### 2) One-vs-Rest strategy (step-by-step)

* Suppose dataset has **K classes**: $C_1, C_2, \dots, C_K$.

**Step A — Convert into binary problems**

1. Take first class $C_1$ as **positive class** ($y=1$) and **all other classes combined** as negative class ($y=0$).

   * Train a **logistic regression** to predict $C_1$ vs rest.
2. Take second class $C_2$ as **positive**, others as negative. Train another logistic regression.
3. Repeat until all $K$ classes have **their own logistic regression**.

**Step B — Hypothesis for each class**

* Each logistic regression outputs a **probability** that input $x$ belongs to that class:

  $$
  h_{\theta^{(i)}}(x) = P(y = C_i \mid x), \quad i = 1,...,K
  $$

* So after training, we have **K separate functions**:

  $$
  h_{\theta^{(1)}}(x),\ h_{\theta^{(2)}}(x),\ ...,\ h_{\theta^{(K)}}(x)
  $$

---

### 3) Prediction step

* Given a **new input** $x$, we compute **all K probabilities**:

  $$
  P(y=C_1\mid x),\ P(y=C_2\mid x),\ ...,\ P(y=C_K\mid x)
  $$

* **Assign class with maximum probability**:

  $$
  \hat{y} = \arg\max_{i} P(y=C_i \mid x)
  $$

**Memory hook:** “Predict the class your logistic regression is *most confident about*.”

---

### 4) Example (fruit dataset)

Suppose dataset has 3 fruits:

* $C_1$ = Apple
* $C_2$ = Banana
* $C_3$ = Orange

**Step A — Create binary problems**

1. **Apple vs Rest**:

   * Apple → 1, Banana/Orange → 0
   * Train logistic regression → $h_{\theta^{(Apple)}}(x)$
2. **Banana vs Rest**:

   * Banana → 1, Apple/Orange → 0
   * Train logistic regression → $h_{\theta^{(Banana)}}(x)$
3. **Orange vs Rest**:

   * Orange → 1, Apple/Banana → 0
   * Train logistic regression → $h_{\theta^{(Orange)}}(x)$

**Step B — Make prediction for new fruit x**

* Suppose feature vector $x = [color=red, size=medium]$

* Compute probabilities:

  * $h_{\theta^{(Apple)}}(x) = 0.7$
  * $h_{\theta^{(Banana)}}(x) = 0.2$
  * $h_{\theta^{(Orange)}}(x) = 0.1$

* Pick class with maximum probability → Apple (0.7).

---

### 5) Visual diagram (ASCII)

Imagine each logistic regression is a “detector” for one class.

```
Input x --> [Apple logistic regression] --> P(Apple|x) = 0.7
        \
         --> [Banana logistic regression] --> P(Banana|x) = 0.2
        \
         --> [Orange logistic regression] --> P(Orange|x) = 0.1

Prediction: Choose max probability → Apple
```

* Each detector acts independently.
* OvR reduces multiclass into **K binary classifiers**.

---

### 6) Key observations (stepwise)

1. Each classifier is **trained separately**:

   * Only considers “its class vs rest”.
   * Does **not care about relative probability between other classes** during training.
2. **Output probabilities** do not necessarily sum to 1.

   * Example: $0.7 + 0.6 + 0.1 = 1.4 > 1$.
   * During prediction, we only care about the **maximum**.
3. Easy to implement: just reuse binary logistic regression code.

---

### 7) Memory-friendly summary

* **Binary logistic regression**: $y \in \{0,1\}$, one classifier → probability → threshold.
* **Multiclass logistic regression (OvR)**:

  * $K$ classes → $K$ binary classifiers.
  * Each predicts probability for **one class vs rest**.
  * Predict class with **highest probability**.

**Tip:** think of each classifier as a **specialized “yes/no detector”** for its class.

---