# Model (simple linear regression)

Equation (predicted value with a “hat”):
$$
\hat{y} = \beta_0 + \beta_1 x
$$

* (x): input (predictor, independent variable).
* (\hat y): predicted output (estimate of the true (y)).
* (\beta_0): intercept (predicted (\hat y) when (x=0)).
* (\beta_1): slope (change in (\hat y) per unit change in (x)).

# Error (residual) and objective

* Residual (error) for observation (i): (e_i = y_i - \hat y_i = y_i - (\beta_0 + \beta_1 x_i)).
* Ordinary Least Squares (OLS) fits (\beta_0,\beta_1) by minimizing the Sum of Squared Errors (SSE):
  $$
  \text{SSE}(\beta_0,\beta_1) = \sum_{i=1}^n (y_i-\beta_0-\beta_1 x_i)^2
  $$

# Closed-form OLS solution (derivation sketch)

Minimize SSE by setting partial derivatives to zero:
$$
\frac{\partial \text{SSE}}{\partial \beta_0} = -2\sum_{i}(y_i-\beta_0-\beta_1 x_i) = 0
$$
$$
\frac{\partial \text{SSE}}{\partial \beta_1} = -2\sum_{i}x_i(y_i-\beta_0-\beta_1 x_i) = 0
$$
Solve the two normal equations → express in terms of sample means (\bar x,\bar y):
$$
\beta_1 = \dfrac{\sum_{i}(x_i-\bar x)(y_i-\bar y)}{\sum_{i}(x_i-\bar x)^2} \quad\text{(covariance/variance)}
$$
$$
\beta_0 = \bar y - \beta_1 \bar x
$$
where (\bar x = \frac{1}{n}\sum x_i,\ \bar y = \frac{1}{n}\sum y_i).

# Key derived quantities and formulas

* Residuals: (e_i = y_i - \hat y_i).
* SSE (= \sum e_i^2).
* SSR (regression sum of squares) (= \sum(\hat y_i-\bar y)^2).
* SST (total sum of squares) (= \sum(y_i-\bar y)^2).
* (R^2 = \dfrac{\text{SSR}}{\text{SST}} = 1 - \dfrac{\text{SSE}}{\text{SST}}).
* Mean Squared Error (estimate of residual variance): (\displaystyle \text{MSE}=\frac{\text{SSE}}{n-2}) (two parameters estimated).
* Root MSE (RMSE): (\sqrt{\text{MSE}}).
* Standard error of slope: (\displaystyle \operatorname{SE}(\beta_1)=\sqrt{\frac{\text{MSE}}{\sum (x_i-\bar x)^2}}).
* Standard error of intercept: (\displaystyle \operatorname{SE}(\beta_0)=\sqrt{\text{MSE}\left(\frac{1}{n}+\frac{\bar x^2}{\sum(x_i-\bar x)^2}\right)}).
* (t)-statistic for (\beta_j): (t=\beta_j/\operatorname{SE}(\beta_j)) with (df=n-2).
* 95% CI for (\beta_j): (\beta_j \pm t_{0.975,n-2}\cdot\operatorname{SE}(\beta_j)).
* Prediction for new input (x^*): (\hat y^*=\beta_0+\beta_1 x^*).

  * SE of mean response: (\sqrt{\text{MSE}\left(\frac{1}{n}+\frac{(x^*-\bar x)^2}{\sum(x_i-\bar x)^2}\right)}).
  * SE of individual prediction: (\sqrt{\text{MSE}\left(1+\frac{1}{n}+\frac{(x^*-\bar x)^2}{\sum(x_i-\bar x)^2}\right)}).
  * 95% prediction interval: (\hat y^* \pm t_{0.975,n-2}\cdot\text{SE}_{\text{pred}}).

# Real numerical example — step-by-step (digit-by-digit arithmetic)

Dataset (hours studied (x) → exam score (y)), (n=6):

|  i | (x_i) | (y_i) |
| -: | :---: | :---: |
|  1 |   1   |   6   |
|  2 |   2   |   8   |
|  3 |   3   |   11  |
|  4 |   4   |   13  |
|  5 |   5   |   15  |
|  6 |   6   |   18  |

1. Means:
   $$
   \bar x = \frac{1+2+3+4+5+6}{6} = \frac{21}{6} = 3.5
   $$
   $$
   \bar y = \frac{6+8+11+13+15+18}{6} = \frac{71}{6} \approx 11.833333333333334
   $$

2. Build table of deviations, squares, cross-products:

|  i | (x_i) | (y_i) | (x_i-\bar x) |           (y_i-\bar y)          | ((x_i-\bar x)^2) | ((x_i-\bar x)(y_i-\bar y)) |
| -: | :---: | :---: | :----------: | :-----------------------------: | :--------------: | :------------------------: |
|  1 |   1   |   6   |  1−3.5=−2.5  | 6−11.8333333=−5.833333333333334 |       6.25       |     14.583333333333334     |
|  2 |   2   |   8   |     −1.5     |        −3.833333333333334       |       2.25       |            5.75            |
|  3 |   3   |   11  |     −0.5     |        −0.833333333333334       |       0.25       |     0.4166666666666667     |
|  4 |   4   |   13  |      0.5     |        1.166666666666666        |       0.25       |      0.583333333333333     |
|  5 |   5   |   15  |      1.5     |        3.1666666666666665       |       2.25       |            4.75            |
|  6 |   6   |   18  |      2.5     |        6.166666666666666        |       6.25       |     15.416666666666666     |

Now sums:
$$
\sum (x_i-\bar x)^2 = 6.25+2.25+0.25+0.25+2.25+6.25 = 17.5
$$
$$
\sum (x_i-\bar x)(y_i-\bar y) = 14.583333333333334+5.75+0.4166666666666667+0.583333333333333+4.75+15.416666666666666 = 41.5
$$

3. Slope and intercept:
   $$
   \beta_1 = \frac{41.5}{17.5} = \frac{83/2}{35/2} = \frac{83}{35} \approx 2.3714285714285714
   $$
   $$
   \beta_0 = \bar y - \beta_1 \bar x = 11.833333333333334 - 2.3714285714285714 \times 3.5 \approx 3.533333333333333
   $$

4. Predicted (\hat y_i) and residuals (e_i):
   $$
   \hat y_i = \beta_0 + \beta_1 x_i
   $$
   Calculated values (rounded display):

|  i | (x_i) |     (\hat y_i)     |  (e_i=y_i-\hat y_i)  |
| -: | :---: | :----------------: | :------------------: |
|  1 |   1   |  5.904761904761905 |  0.0952380952380949  |
|  2 |   2   |  8.276190476190475 |  -0.2761904761904752 |
|  3 |   3   | 10.647619047619047 |  0.3523809523809529  |
|  4 |   4   | 13.019047619047619 | -0.01904761904761898 |
|  5 |   5   |  15.39047619047619 | -0.39047619047619087 |
|  6 |   6   |  17.76190476190476 |  0.2380952380952408  |

5. SSE, SSR, SST:

* Squared residuals: (e_i^2) ≈ [0.009070294785, 0.076281179138, 0.124172335601, 0.000362811791, 0.152471655329, 0.056689342404]
* (\text{SSE} = \sum e_i^2 \approx 0.4190476190476204).

Compute SST and SSR:
$$
\text{SST} = \sum(y_i-\bar y)^2 \approx 98.83333333333333
$$
$$
\text{SSR} = \sum(\hat y_i-\bar y)^2 = \text{SST} - \text{SSE} \approx 98.41428571428568
$$

6. (R^2):
   $$
   R^2 = 1 - \frac{\text{SSE}}{\text{SST}} = 1 - \frac{0.4190476190476204}{98.83333333333333} \approx 0.99576
   $$
   (about 99.576% of variance explained)

7. Residual variance, RMSE:
   $$
   \text{MSE} = \frac{\text{SSE}}{n-2} = \frac{0.4190476190476204}{6-2} = \frac{0.4190476190476204}{4} \approx 0.1047619047619051
   $$
   $$
   \text{RMSE} = \sqrt{\text{MSE}} \approx 0.3236694374850754
   $$

8. Standard errors of coefficients:
   $$
   \operatorname{SE}(\beta_1) = \sqrt{\frac{\text{MSE}}{\sum(x_i-\bar x)^2}} = \sqrt{\frac{0.1047619047619051}{17.5}} \approx 0.07737179432986642
   $$
   $$
   \operatorname{SE}(\beta_0) = \sqrt{ \text{MSE} \left( \frac{1}{n} + \frac{ \bar x^2 }{ \sum(x_i-\bar x)^2 } \right) } \approx 0.3013198479915505
   $$

9. (t)-statistic for (\beta_1) and 95% CI (df = (n-2=4), (t_{0.975,4}\approx 2.776)):
   $$
   t = \frac{ \beta_1 }{ \operatorname{SE}(\beta_1)} \approx \frac{2.3714285714285714}{0.07737179432986642} \approx 30.65
   $$
   Very large (t) → coefficient highly significant (p-value ≪ 0.01).
   95% CI for (\beta_1):
   $$
   \beta_1 \pm t_{0.975,4} \cdot \operatorname{SE}(\beta_1) \approx 2.37143 \pm 2.776 \times 0.0773718
   $$
   $$
   \text{CI}_{95\%}(\beta_1)\approx (2.15661,\;2.58625)
   $$

10. Prediction example for new input (x^*=7):
    $$
    \hat y^* = \beta_0 + \beta_1 \times 7 \approx 3.5333333+2.3714285714\times7 \approx 20.133333333333333
    $$
    SE of mean response at (x^*=7):
    $$
    \text{SE}_{\text{mean}} = \sqrt{ \text{MSE} \left( \frac{1}{n} + \frac{ (7-\bar x)^2 }{ \sum (x_i - \bar x)^2 } \right) } \approx 0.3013198479915505
    $$
    95% CI for mean response:
    $$
    (19.296735316278248,\;20.969931350388418)
    $$
    SE of individual prediction:
    $$
    \text{SE}_{\text{pred}} = \sqrt{ \text{MSE} \left( 1 + \frac{1}{n} + \frac{ (7-\bar x)^2 }{ \sum (x_i-\bar x)^2 } \right) } \approx 0.44221663871405403
    $$
    95% prediction interval (wider; includes individual variability):
    $$
    (18.905543111338673,\;21.361123555327993)
    $$

