# **Regularization**

---

## **1. Core Idea**

Regularization is a **technique to prevent overfitting** by **penalizing overly complex models**. It encourages the model to **focus on true underlying patterns** rather than memorizing noise in the training dataset.

Mathematically, it modifies the **loss function**:

$$
\text{Loss}_{\text{regularized}} = \text{Loss}_{\text{original}} + \lambda \cdot \text{Penalty}
$$

* ( \lambda ) → regularization strength (controls how much we penalize complexity)
* **Penalty** → measure of model complexity (depends on type of regularization)

---

## **2. Types of Regularization**

| Type            | Penalty Term   | Description                       | Use Case                                           |                                |                                   |
| --------------- | -------------- | --------------------------------- | -------------------------------------------------- | ------------------------------ | --------------------------------- |
| **L1 (Lasso)**  | \( \sum |w_i| \)        | Adds absolute value of weights     | Feature selection (sparse models)                  |                                |                                   |
| **L2 (Ridge)**  | \( \sum w_i^2 \)        | Adds squared magnitude of weights | Smooth, small weights, prevents large coefficients |                                |                                   |
| **Elastic Net** | \( \alpha \sum |w_i| + (1-\alpha) \sum w_i^2 \) | Combines L1 and L2             | Balances sparsity and smoothness                   |                                |                                   |

---

## **3. Intuition**

* High complexity → large weights → model fits noise → **overfitting**
* Regularization → penalizes large weights → model simpler → better **generalization**

```
Without Regularization:   w1=100, w2=50 → overfit
With L2 Regularization:   w1=1.5, w2=0.8 → generalize
```

---

## **4. Visualization (Mental Model)**

```
Loss vs Model Complexity
|\
| \
|  \
|   \
|    \   <-- Regularization
|     \
|      \
------------------> Complexity
```

* **Without regularization:** Loss decreases continuously, but model may overfit
* **With regularization:** Penalizes complexity, stops overfitting

---

## **5. Key Notes**

* Regularization is **hyperparameter-driven** → λ or α must be tuned (usually via validation dataset)
* Prevents **overfitting**
* Can help with **feature selection** (L1)
* Works for most algorithms: Linear Regression, Logistic Regression, Neural Networks, etc.

---

## **6. Memory Hooks**

* **L1 → “Lasso = Zero out unimportant features”**
* **L2 → “Ridge = Smooth hills, no spikes”**
* **Purpose → Keep it simple, stupid (KISS)**

---

