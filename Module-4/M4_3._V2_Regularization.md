# Regularization: Practical and Mathematical Perspective with Visuals

---

## **1. Mathematical Formulation**

Regularization adds a **penalty term** to the original loss function to control model complexity and prevent overfitting.

### **A. Linear/Logistic Regression Loss**

**Linear Regression (MSE):**
$$
J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})^2
$$

**Logistic Regression (Cross-Entropy Loss):**
$$
J(\theta) = - \frac{1}{m} \sum_{i=1}^{m} \Big[ y^{(i)} \log h_\theta(x^{(i)}) + (1-y^{(i)}) \log(1-h_\theta(x^{(i)})) \Big]
$$

---

### **B. Adding Regularization**

**L2 (Ridge) Regularization:**
$$
J_{reg}(\theta) = J(\theta) + \frac{\lambda}{2m} \sum_{j=1}^{n} \theta_j^2
$$

**L1 (Lasso) Regularization:**
$$
J_{reg}(\theta) = J(\theta) + \frac{\lambda}{m} \sum_{j=1}^{n} |\theta_j|
$$

Where:

* \( \lambda \) = regularization parameter (strength of penalty)
* \( \theta_j \) = weight of feature \( j \)
* \( n \) = number of features

---

## **2. Intuition**

| Type            | Effect on Weights       | Practical Use                 |
| --------------- | ----------------------- | ----------------------------- |
| **L1 (Lasso)**  | Some weights → 0        | Feature selection             |
| **L2 (Ridge)**  | Weights shrink smoothly | Prevents large spikes         |
| **Elastic Net** | Mix of L1 + L2          | Balance sparsity + smoothness |

---

## **3. Visualization of Regularization Effect**

### **A. Weight Shrinkage Example**

```
Weights
|         L2
|       / 
|      /      L1
|     /
|----/----------------> Features
```

* L1 pushes some weights **exactly to 0**
* L2 shrinks **all weights gradually**

---

### **B. Decision Boundary Example (Logistic Regression)**

```
Without Regularization:    Overfits noise
    *
   * *
  *   *   
 -----------
Predictions perfectly fit training data but may fail on new data

With L2 Regularization:     Smooth decision boundary
    *
   *   *
  *     *
 -----------
Generalizes better to unseen data
```

---

## **4. Practical Implementation (Python)**

```python
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_breast_cancer
from sklearn.metrics import accuracy_score

# Load dataset
X, y = load_breast_cancer(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Logistic Regression with L2 Regularization
model_ridge = LogisticRegression(penalty='l2', C=1.0, solver='liblinear')
model_ridge.fit(X_train, y_train)
y_pred = model_ridge.predict(X_test)
print("Accuracy with L2:", accuracy_score(y_test, y_pred))

# Logistic Regression with L1 Regularization
model_lasso = LogisticRegression(penalty='l1', C=1.0, solver='liblinear')
model_lasso.fit(X_train, y_train)
y_pred_lasso = model_lasso.predict(X_test)
print("Accuracy with L1:", accuracy_score(y_test, y_pred_lasso))
```

---

## **5. Summary**

* Regularization prevents **overfitting** by penalizing large or complex weights
* **L1** → sparsity (some weights = 0)
* **L2** → smooth shrinkage (all weights reduced)
* **λ** controls penalty strength → needs tuning via **validation dataset**
* Visual cues: **L1 → sparse spikes**, **L2 → smooth curve**, **unregularized → noisy overfit**

---
