# What is Neural Network?

A **neural network** is a computational model inspired by the structure and functioning of the human brain. It is designed to recognize complex patterns and relationships in data through interconnected processing units called **neurons** or **nodes**. Neural networks form the foundation of **deep learning**, a subset of machine learning.

---

### 1. **Structure of a Neural Network**

A typical neural network consists of three types of layers:

```
Input Layer → Hidden Layer(s) → Output Layer
```

* **Input Layer:**
  Accepts raw data (numerical, image pixels, audio signals, etc.). Each neuron here represents one feature of the input data.

* **Hidden Layers:**
  Perform intermediate computations. Each neuron applies a mathematical transformation using a **weighted sum** of inputs and an **activation function** to introduce non-linearity.
  These layers learn abstract patterns (e.g., shapes, tones, words).

* **Output Layer:**
  Produces the final prediction or classification result (e.g., “cat” or “dog,” “positive” or “negative”).

---

### 2. **Mathematical Representation**

Each neuron operates as:

$$
y = f\left(\sum_{i=1}^{n} w_i x_i + b\right)
$$

Where:

* ( x_i ): input values
* ( w_i ): corresponding weights
* ( b ): bias term (adjusts the output along with weights)
* ( f ): activation function (e.g., sigmoid, ReLU, tanh)
* ( y ): output from the neuron

---

### 3. **Learning Process**

Neural networks learn by adjusting weights to minimize the difference between predicted and actual outputs. This process is called **training** and involves:

* **Forward Propagation:**
  Inputs move through layers to generate predictions.

* **Loss Function:**
  Measures the error (e.g., Mean Squared Error, Cross Entropy).

* **Backward Propagation (Backpropagation):**
  The algorithm calculates how much each neuron contributed to the error and updates weights using **gradient descent**.

---

### 4. **Example**

Suppose we build a neural network to recognize handwritten digits (0–9):

1. **Input Layer:** 784 neurons (for 28×28 pixel grayscale image).
2. **Hidden Layers:** 2–3 layers that extract edges, shapes, and digit patterns.
3. **Output Layer:** 10 neurons (one for each digit).
4. **Training:** The network adjusts weights based on thousands of labeled images until it correctly predicts digits.

---

### 5. **Applications**

Neural networks are widely applied in:

* **Speech Recognition:** Converting voice to text (e.g., Siri, Google Assistant).
* **Image Recognition:** Face identification, object detection, medical imaging.
* **Healthcare:** Disease prediction, drug discovery, personalized treatments.
* **Marketing:** Customer segmentation, recommendation systems, sales forecasting.
* **Finance:** Fraud detection, stock prediction, credit scoring.
* **Autonomous Systems:** Self-driving cars, robotics control, drone navigation.

---

### 6. **Types of Neural Networks**

| Type                                     | Description                                            | Example Application           |
| ---------------------------------------- | ------------------------------------------------------ | ----------------------------- |
| **Feedforward Neural Network (FNN)**     | Basic architecture; information flows in one direction | Simple classification         |
| **Convolutional Neural Network (CNN)**   | Uses convolution layers to detect spatial features     | Image and video recognition   |
| **Recurrent Neural Network (RNN)**       | Has feedback loops to handle sequential data           | Speech, language, time series |
| **Generative Adversarial Network (GAN)** | Two networks compete to generate realistic data        | Deepfakes, image generation   |
| **Transformer Network**                  | Uses self-attention for parallel sequence processing   | ChatGPT, language translation |

---

### 7. **Analogy with the Human Brain**

| Human Brain                 | Neural Network                        |
| --------------------------- | ------------------------------------- |
| Neurons                     | Artificial neurons (nodes)            |
| Synapses                    | Weights connecting nodes              |
| Learning through experience | Weight adjustment via backpropagation |
| Brain’s activation          | Activation functions (ReLU, Sigmoid)  |

---

### 8. **Visualization**

```
[Input Data]
     ↓
[Input Layer: x1, x2, x3, ...]
     ↓
[Hidden Layer 1: feature detection]
     ↓
[Hidden Layer 2: pattern recognition]
     ↓
[Output Layer: final decision/prediction]
```

---

### 9. **Key Advantages**

* Learns complex, non-linear relationships.
* Adapts and improves automatically with more data.
* Performs well in unstructured data (images, speech, text).

### 10. **Limitations**

* Requires large datasets and high computational power.
* Difficult to interpret (black-box nature).
* Susceptible to overfitting without regularization.

---

---

---

---

---

---

# Single Neuron Working with Two Inputs (x₁, x₂), Weights (θ₁, θ₂), and Sigmoid Activation Function

A **single neuron** (or node) is the basic computational unit of a neural network. It receives one or more **inputs**, multiplies them by **weights**, adds a **bias**, and then applies an **activation function** (e.g., sigmoid) to produce the **output**.

---

### 1. **Mathematical Model**

Given two inputs ( x_1 ) and ( x_2 ) with respective weights ( \theta_1 ) and ( \theta_2 ):

$$
z = \theta_1 x_1 + \theta_2 x_2 + b
$$

Then the neuron’s output ( y ) is:

$$
y = f(z) = f(\theta_1 x_1 + \theta_2 x_2 + b)
$$

where ( f ) is the **activation function** — here, the **sigmoid**.

---

### 2. **Sigmoid Activation Function**

The **sigmoid function** squashes the input value into a range between 0 and 1, making it useful for binary outputs (e.g., yes/no, 0/1):

$$
f(z) = \frac{1}{1 + e^{-z}}
$$

**Properties:**

* Output always between 0 and 1
* Smooth and differentiable (useful for gradient-based learning)
* As ( z \to +\infty ), ( f(z) \to 1 ); as ( z \to -\infty ), ( f(z) \to 0 )

---

### 3. **Step-by-Step Example**

Assume:

* ( x_1 = 0.5 )
* ( x_2 = 0.8 )
* ( \theta_1 = 0.4 )
* ( \theta_2 = 0.6 )
* ( b = 0.1 )

**Step 1:** Compute weighted sum
$$
z = (0.4)(0.5) + (0.6)(0.8) + 0.1 = 0.2 + 0.48 + 0.1 = 0.78
$$

**Step 2:** Apply sigmoid activation
$$
y = \frac{1}{1 + e^{-0.78}} \approx 0.685
$$

So the neuron outputs **y = 0.685** (interpreted as a probability).

---

### 4. **Diagram (ASCII Representation)**

```
        x1 ----->(θ1=0.4)---\
                              \
                               [Σ]----> z = θ1x1 + θ2x2 + b
                              /
        x2 ----->(θ2=0.6)---/
                              \
                               [Activation f(z)=sigmoid(z)]
                                        ↓
                                    Output y
```

---

### 5. **Interpretation**

* Each **input (x₁, x₂)** represents a feature (like height, weight, pixel intensity).
* Each **weight (θ₁, θ₂)** determines how important that feature is.
* The **bias (b)** shifts the activation threshold.
* The **activation function (sigmoid)** introduces non-linearity, allowing the neuron to model complex relationships.

If the output ( y > 0.5 ), it might correspond to **class 1**, otherwise **class 0** — depending on the problem.

---

### 6. **General Formula for Multiple Inputs**

For ( n ) inputs:

$$
y = f(\theta_1 x_1 + \theta_2 x_2 + \ldots + \theta_n x_n + b)
$$

This formula defines one neuron’s computation, and multiple such neurons connected together form a **neural network**.

---

### 7. **Summary Table**

| Term       | Meaning              | Example Value |
| ---------- | -------------------- | ------------- |
| (x_1, x_2) | Input features       | 0.5, 0.8      |
| (θ_1, θ_2) | Weights              | 0.4, 0.6      |
| (b)        | Bias                 | 0.1           |
| (z)        | Weighted sum         | 0.78          |
| (f(z))     | Activation (sigmoid) | 0.685         |
| (y)        | Final neuron output  | 0.685         |

---

---

---

---

---

---

