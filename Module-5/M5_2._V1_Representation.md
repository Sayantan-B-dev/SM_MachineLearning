# Representation of a Neural Network

A neural network can be represented **graphically, mathematically, and structurally** to show how data flows through layers, how neurons are connected via weights, and how activations are computed.

---

### 1. **Graphical (Layered) Representation**

**Simple feedforward network:**

```
          Input Layer
        x1   x2   x3
         \    |    /
          \   |   /
          Hidden Layer 1
          h1   h2   h3
           \   |   /
            \  |  /
          Hidden Layer 2
            h4   h5
             \   /
              \ /
           Output Layer
                y
```

* **Nodes (circles)** represent **neurons**.
* **Lines/arrows** represent **weights (connections)**. Each line has a weight (w_{ij}).
* Data flows from **input → hidden layers → output** (feedforward).

---

### 2. **Mathematical Representation**

For a **single neuron**:

$$
y = f\Big(\sum_{i=1}^{n} w_i x_i + b\Big)
$$

For a **layer of neurons**, using vector/matrix notation:

$$
\mathbf{z}^{(l)} = \mathbf{W}^{(l)} \mathbf{a}^{(l-1)} + \mathbf{b}^{(l)}
$$

$$
\mathbf{a}^{(l)} = f(\mathbf{z}^{(l)})
$$

Where:

* ( \mathbf{a}^{(0)} = \mathbf{x} ) = input vector
* ( \mathbf{W}^{(l)} ) = weight matrix connecting layer (l-1) to (l)
* ( \mathbf{b}^{(l)} ) = bias vector for layer (l)
* ( f ) = activation function (sigmoid, ReLU, tanh)
* ( \mathbf{a}^{(l)} ) = output vector of layer (l)

---

### 3. **Network as a Weighted Graph**

* Nodes: neurons
* Edges: weights
* Each neuron computes:

$$
\text{Output of neuron } j = f\Big(\sum_i w_{ij} x_i + b_j\Big)
$$

* Entire network = directed graph with weighted edges, input nodes on one side, output nodes on the other.

**Example weight mapping (2 inputs → 2 hidden → 1 output):**

| From → To | Weight |
| --------- | ------ |
| x1 → h1   | w11    |
| x2 → h1   | w21    |
| x1 → h2   | w12    |
| x2 → h2   | w22    |
| h1 → y    | w31    |
| h2 → y    | w32    |

---

### 4. **ASCII Representation (Compact)**

```
Inputs: x1, x2
          │
          ▼
        [Hidden Layer]
      h1       h2
       │       │
       ▼       ▼
        [Output]
           y
```

Each arrow = weight (w_{ij}).
Each hidden/output node applies **activation function** on the weighted sum.

---

### 5. **Vectorized Representation**

For a mini-batch of inputs (X) (n samples × m features):

1. Hidden layer pre-activation:
   $$
   Z^{(1)} = X W^{(1)} + B^{(1)}
   $$
2. Hidden layer activation:
   $$
   A^{(1)} = f(Z^{(1)})
   $$
3. Output pre-activation:
   $$
   Z^{(2)} = A^{(1)} W^{(2)} + B^{(2)}
   $$
4. Output activation:
   $$
   \hat{Y} = f(Z^{(2)})
   $$

---

### 6. **Summary Table of Representation**

| Representation Type | Description                               | Example                      |
| ------------------- | ----------------------------------------- | ---------------------------- |
| Graphical/Diagram   | Nodes + weighted edges                    | Layered diagram with arrows  |
| Mathematical        | Equations for weighted sums & activations | \( y = f(\sum w_i x_i + b) \)  |
| Vectorized/Matrix   | Efficient computation for batches         | \( Z = X W + B, A = f(Z) \)    |
| Weighted Graph      | Directed graph                            | Nodes=neurons, Edges=weights |

---

### 7. **Key Points**

* **Neurons** = processing units
* **Weights** = connection strength
* **Biases** = shift the activation
* **Activation function** = introduces non-linearity
* **Layers** = input, hidden, output
* **Network** = composition of multiple layers & neurons forming complex mappings

---

---

---

---

---

---

