# ðŸ“˜ Intuition in Linear Regression

---

## 1. Basic Idea of Linear Regression

* **Goal:** We want to **predict a continuous value** (e.g., predicting house price, marks, salary) based on input(s).
* **Input variable(s):**

  * Called **feature(s)** or **independent variables (x)**.
  * Example: size of house in square feet.
* **Output variable:**

  * Called **dependent variable (y)**.
  * Example: price of house.

ðŸ‘‰ **Key Concept:**
We assume there exists some **linear relationship** between input(s) and output, i.e.,

$$
y \approx h(x)
$$

where $h(x)$ is our **hypothesis (model equation).**

---

## 2. Dataset Representation

* We usually store data in a **table format (matrix form):**

| Size of house (sq.ft) $x$ | Price (â‚¹ in Lakhs) $y$ |
| ------------------------- | ---------------------- |
| 1000                      | 50                     |
| 1500                      | 70                     |
| 2000                      | 90                     |

* **m = number of rows (examples/observations).**
  In the above, $m = 3$.

* **n = number of columns (features/inputs).**
  In the above, $n = 1$ (only size of house).

ðŸ‘‰ If we add more columns (e.g., location, number of rooms), then $n > 1$.

---

## 3. Hypothesis (Model Equation)

The model tries to fit a **line (or hyperplane)** that explains the relationship.

* **Single feature (univariate):**

$$
h(x) = \theta_0 + \theta_1 x
$$

where:

* $\theta_0$ = intercept (like **c** in $y = mx + c$)

* $\theta_1$ = slope (like **m**)

* **Multiple features (multivariate):**

$$
h(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \dots + \theta_n x_n
$$

ðŸ‘‰ Thetas ($\theta$) are called **parameters** or **weights**, and the learning algorithm will adjust them during training.

---

## 4. Training Concept

* **Training Set** = collection of (x, y) pairs.
* **Learning Algorithm** = optimization process that adjusts $\theta$ values to best fit the data.
* **Hypothesis (h):** The final equation representing the relationship.

**Pipeline:**

$$
\text{Training Data} \;\;\;\longrightarrow\;\;\; \text{Learning Algorithm} \;\;\;\longrightarrow\;\;\; h(x) \; (\text{model})
$$

---

## 5. Geometric Intuition

* With **1 feature** â†’ The model is a **straight line** in 2D space.
* With **2 features** â†’ The model is a **plane** in 3D space.
* With **n features** â†’ The model is a **hyperplane** in (n+1)-dimensional space.

---

## 6. Example (Step-by-Step)

Suppose you have data:

| Hours studied (x) | Marks obtained (y) |
| ----------------- | ------------------ |
| 1                 | 40                 |
| 2                 | 50                 |
| 3                 | 65                 |

We want a model:

$$
h(x) = \theta_0 + \theta_1 x
$$

* Suppose after training, algorithm finds:
  $\theta_0 = 30$, $\theta_1 = 12$

* Model becomes:

  $$
  h(x) = 30 + 12x
  $$

* Predictions:

  * If $x = 2$: $h(2) = 30 + 24 = 54$ marks
  * If $x = 5$: $h(5) = 30 + 60 = 90$ marks

ðŸ‘‰ This is how linear regression uses input $x$ to predict output $y$.

---

## 7. Comparison with Line Equation

* Standard line:

  $$
  y = mx + c
  $$
* Hypothesis in regression:

  $$
  h(x) = \theta_1 x + \theta_0
  $$

So:

* Slope ($m$) â†’ $\theta_1$
* Intercept ($c$) â†’ $\theta_0$

---

âœ… **Exam-style Summary Points:**

* Linear regression is used to predict **continuous output**.
* Relationship assumed: $y \approx h(x) = \theta_0 + \theta_1 x$.
* Training data: $m$ rows (examples), $n$ features (inputs).
* Multiple features â†’ multivariate regression with hypothesis $h(x) = \theta_0 + \sum \theta_i x_i$.
* Geometric meaning: line (2D), plane (3D), hyperplane (nD).
* Parameters ($\theta$) are learned using training data.

---

