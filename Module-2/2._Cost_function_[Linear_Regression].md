---

# üìò Cost Function in Linear Regression

---

## 1. Effect of Changing Parameters

* **Intercept ($\theta_0$)**

  * Controls the **vertical shift** of the line.
  * Increasing $\theta_0$ moves the line **upward**.
  * Decreasing $\theta_0$ moves the line **downward**.
  * Example: If predicting house price, higher $\theta_0$ means the **baseline price** increases regardless of size.

* **Slope ($\theta_1$)**

  * Controls the **tilt/steepness** of the line.
  * Larger positive $\theta_1$ ‚Üí line rises more steeply.
  * Negative $\theta_1$ ‚Üí line goes downward.
  * Example: Higher slope in house price model means **price increases faster** with size.

üëâ Together, $\theta_0$ and $\theta_1$ decide **where the line lies and how it slants**.

---

## 2. Error (Difference Between Prediction and Reality)

* Prediction by model:

  $$
  h(x) = \theta_0 + \theta_1 x
  $$

* Actual observed value:

  $$
  y
  $$

* **Error / Residual for one point:**

  $$
  \text{Error} = y - h(x)
  $$

* Why not just use raw error?

  * Some errors are positive (model predicts lower than actual).
  * Some errors are negative (model predicts higher than actual).
  * If we sum directly, they cancel out.

---

## 3. Squared Error

* To avoid cancellation, we square the error:

  $$
  (h(x) - y)^2
  $$

* Properties:

  * Always non-negative.
  * Penalizes **larger errors** more heavily.
  * Example: Error = 5 ‚Üí squared error = 25. Error = 10 ‚Üí squared error = 100.

üëâ This ensures the model pays more attention to big mistakes.

---

## 4. Cost Function (for Entire Dataset)

We want an equation that **measures total error across all training points**.

* Training data: $m$ examples $(x^{(i)}, y^{(i)})$.

* **Cost Function (Mean Squared Error ‚Äì MSE):**

  $$
  J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{i=1}^{m} \Big( h(x^{(i)}) - y^{(i)} \Big)^2
  $$

* Explanation:

  * $h(x^{(i)})$: prediction for i-th input.
  * $y^{(i)}$: actual true output.
  * Squared difference ‚Üí squared error.
  * Summation over all training points ‚Üí total error.
  * Divide by $2m$ ‚Üí average error (factor $1/2$ is for math convenience in differentiation).

---

## 5. Why This Formula?

* **Real-life intuition:**

  * Imagine predicting house prices. If predicted price is ‚Çπ60 lakh but true price is ‚Çπ50 lakh, error = 10 lakh.
  * Another house: predicted 40 lakh, true 50 lakh, error = ‚Äì10 lakh.
  * Raw sum ‚Üí 0, but clearly both are wrong.
  * Squaring ensures both contribute equally, and **bigger mistakes hurt more.**

* **Why mean?**

  * Different datasets can have different sizes. Averaging makes error measure independent of dataset size.

* **Why square instead of absolute?**

  * Squaring gives smooth curves (differentiable), making optimization with calculus easier.
  * Absolute value leads to sharp edges ‚Üí harder to optimize.

---

## 6. Residuals

* **Residual = Actual ‚Äì Predicted = $y - h(x)$**
* Represented as vertical distance from each point to the regression line.
* The objective of linear regression = **minimize the sum of squared residuals (SSR).**

Graphically:

* If points are scattered around, regression line is placed so that total vertical ‚Äúgaps‚Äù (residuals) squared is minimized.

---

## 7. Minimization Objective

* **We want:**

  * Small $J(\theta_0, \theta_1)$.
  * That means small squared residuals.
  * That means line fits training data as closely as possible.

* **How achieved:**

  * By adjusting $\theta_0, \theta_1$ (parameters).
  * Process done by algorithms like **Gradient Descent** or solving **Normal Equation**.

---

## 8. Step-by-Step Example (Manual)

Training data:

| Hours studied (x) | Marks (y) |
| ----------------- | --------- |
| 1                 | 40        |
| 2                 | 50        |
| 3                 | 65        |

Suppose hypothesis:

$$
h(x) = 30 + 10x
$$

Predictions:

* For $x=1$: $h(1) = 40$, error = $40-40=0$
* For $x=2$: $h(2) = 50$, error = $50-50=0$
* For $x=3$: $h(3) = 60$, error = $65-60=5$

Squared errors: $0^2, 0^2, 5^2 = 25$

Cost function:

$$
J(\theta_0,\theta_1) = \frac{1}{2m}\sum (h(x)-y)^2 = \frac{1}{2 \cdot 3} (0+0+25) = \frac{25}{6} \approx 4.17
$$

üëâ Lower value of $J$ = better fit.

---

## 9. Exam-style Quick Notes

* Error = $y - h(x)$
* Residual = observed ‚Äì predicted
* Squared error prevents cancellation + penalizes large mistakes
* Cost function (MSE):

  $$
  J(\theta) = \frac{1}{2m}\sum (h(x)-y)^2
  $$
* Objective = minimize $J$ ‚Üí best values of parameters ($\theta$).
* Graph intuition: line chosen so that squared vertical gaps between points and line are minimized.

---
