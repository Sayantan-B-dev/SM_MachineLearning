# 🌍 Multivariate Linear Regression

### 🔹 Hypothesis function

For **one variable** (simple regression):

$$
h_\theta(x) = \theta_0 + \theta_1 x
$$

For **multiple variables** ($n$ features):

$$
h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \cdots + \theta_n x_n
$$

To make math elegant, we define:

* $x_0 = 1$ (a dummy feature for bias/intercept)
* Feature vector:

$$
x =
\begin{bmatrix}
x_0 \\
x_1 \\
x_2 \\
\vdots \\
x_n
\end{bmatrix}
$$

* Parameter vector:

$$
\theta =
\begin{bmatrix}
\theta_0 \\
\theta_1 \\
\theta_2 \\
\vdots \\
\theta_n
\end{bmatrix}
$$

Then:

$$
h_\theta(x) = \theta^T x
$$

which is **vectorized hypothesis** (very common in ML).

---

# 🧮 Cost Function (MSE)

The cost is still **Mean Squared Error (MSE)**:

$$
J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} \Big( h_\theta(x^{(i)}) - y^{(i)} \Big)^2
$$

* $m$ = number of training examples
* $x^{(i)}$ = feature vector of the $i$-th example
* $y^{(i)}$ = true output

The goal: **minimize $J(\theta)$ by adjusting all $\theta_j$.**

---

# 🌀 Gradient Descent Update Rule

For each parameter $\theta_j$:

$$
\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta)
$$

👉 Derivative result:

$$
\theta_j := \theta_j - \alpha \cdot \frac{1}{m} \sum_{i=1}^{m} \Big( h_\theta(x^{(i)}) - y^{(i)} \Big) x_j^{(i)}
$$

So:

* For **bias term** ($\theta_0$):

$$
\theta_0 := \theta_0 - \alpha \cdot \frac{1}{m} \sum_{i=1}^{m} \Big( h_\theta(x^{(i)}) - y^{(i)} \Big) \cdot 1
$$

* For **feature weights** ($\theta_1, \theta_2, ...$):

$$
\theta_j := \theta_j - \alpha \cdot \frac{1}{m} \sum_{i=1}^{m} \Big( h_\theta(x^{(i)}) - y^{(i)} \Big) x_j^{(i)}
$$

---

# 📊 Why Update All Parameters Simultaneously?

Suppose you update **$\theta_0$** first, then use the updated value while computing **$\theta_1$**.

➡️ Problem: The updates become **dependent**, and $\theta_1$ is based on a partially changed cost function.
➡️ This disturbs the path to the **true gradient direction** and can cause oscillation or divergence.

✅ Correct method: Compute all updates **in parallel** using the *old values of all $\theta_j$*. After computing the updates, apply them simultaneously.

This ensures we’re **always moving in the correct descent direction**.

---

# 🌄 Local vs Global Minima in Multivariate

In **linear regression**, the cost function $J(\theta)$ is **convex** (a "bowl" shape in higher dimensions).

* Convex ⇒ only **one global minima**, no local minima traps.
* This is why linear regression is easier compared to neural networks.

**3D Bowl-shaped cost function for 2 parameters:**

```
       J(θ)
        |
        |        .
        |      .   .
        |   .        .
        |.______________  θ1
       /
      /
     θ0
```

The **bottom of the bowl** = global minima.

---

# 🖼️ Clean Diagram: Vectorized Gradient Descent

```
x = [x0, x1, x2, ..., xn]ᵀ   (feature vector)
θ = [θ0, θ1, θ2, ..., θn]ᵀ   (parameter vector)

Hypothesis:
hθ(x) = θᵀx

Gradient descent update (vectorized):
θ := θ - α * (1/m) * Xᵀ (Xθ - y)
```

Where:

* $X$ = design matrix (m × n) of features
* $y$ = output vector
* $Xθ - y$ = vector of residuals

---

✅ Summary (easy to memorize):

* Hypothesis: $h_\theta(x) = \theta^T x$
* Cost: $J(\theta) = \frac{1}{2m}\sum (h_\theta(x)-y)^2$
* Update: $\theta_j := \theta_j - \alpha \frac{1}{m}\sum (h_\theta(x)-y)x_j$
* Update all $\theta$ simultaneously using the same cost
* Cost is convex (guaranteed global minima, no traps)

---

# All together:
# 📘 Linear Regression — Full Study Notes

---

## 1. 🌍 What is Linear Regression?

Linear regression is a **supervised learning algorithm** used for predicting a continuous output (numeric value) from one or more input features.

* Goal: Find a **linear relationship** between input(s) and output.
* Applications:

  * Predicting house prices 🏠
  * Salary prediction 💰
  * Forecasting demand 📈
  * Medical predictions (e.g., blood pressure vs. age, weight)

---

## 2. ✏️ Simple Linear Regression (One Feature)

We want to model:

$$
y \approx h_\theta(x) = \theta_0 + \theta_1 x
$$

* $x$ = input feature (independent variable)
* $y$ = target (dependent variable)
* $\theta_0$ = intercept (bias)
* $\theta_1$ = slope (weight for feature)

👉 It’s like fitting a **straight line** through data points.

---

### 🔹 Geometric Meaning of Parameters

* **Intercept ($\theta_0$)** → vertical shift (moves the line up or down).
* **Slope ($\theta_1$)** → tilt/steepness of the line (how fast y changes with x).

🖼️ Diagram (line fitting idea):

```
y
|
|       *
|   *           *
|       *   *
|______________________ x
       best-fit line
```

---

## 3. 🧮 Cost Function (How good is a line?)

We measure the **error** between predictions and actual values.

* Prediction:

$$
h_\theta(x^{(i)}) = \theta_0 + \theta_1 x^{(i)}
$$

* Error for example $i$:

$$
error^{(i)} = h_\theta(x^{(i)}) - y^{(i)}
$$

* Squared error (avoid negatives):

$$
(error^{(i)})^2 = (h_\theta(x^{(i)}) - y^{(i)})^2
$$

* **Cost function (Mean Squared Error):**

$$
J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})^2
$$

🖼️ Cost function surface (convex bowl):

```
   J(θ0,θ1)
      |
      |        .
      |     .     .
      |   .         .
      |.______________ θ1
     /
    /
   θ0
```

* **Why squared error?**

  * Always positive
  * Penalizes large errors more
  * Smooth, differentiable function (great for calculus optimization)

---

## 4. 🌀 Gradient Descent (Finding the Best Line)

We want values of $\theta_0, \theta_1$ that minimize $J(\theta_0,\theta_1)$.
Instead of guessing, we use **gradient descent**.

### 🔹 Steps:

1. Initialize $\theta_0, \theta_1$ randomly.
2. Repeat until convergence:

   $$
   \theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta_0,\theta_1)
   $$

   where:

   * $\alpha$ = learning rate (step size)
   * $\frac{\partial}{\partial \theta_j} J$ = slope/derivative w\.r.t parameter

👉 For linear regression:

* Update for intercept:

$$
\theta_0 := \theta_0 - \alpha \frac{1}{m}\sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})
$$

* Update for slope:

$$
\theta_1 := \theta_1 - \alpha \frac{1}{m}\sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x^{(i)}
$$

---

### 🔹 Intuition of Gradient Descent

* Derivative = slope of tangent (direction of steepest increase).
* Negative derivative = move opposite to slope to **descend**.
* $\alpha$ (learning rate) controls step size:

  * Too small → very slow
  * Too big → may overshoot or diverge
  * Just right → smooth convergence

🖼️ Graph of cost vs parameter:

```
J(θ)
|
|        *
|     *       *
|   *
| *        minima (best θ)
|___________________ θ
```

---

## 5. 🔢 Multivariate Linear Regression

Now, suppose we have **n features**.

$$
h_\theta(x) = \theta_0 + \theta_1x_1 + \theta_2x_2 + \cdots + \theta_nx_n
$$

### 🔹 Vectorized Form

Define:

$$
x =
\begin{bmatrix}
x_0 \\
x_1 \\
x_2 \\
\vdots \\
x_n
\end{bmatrix}, \quad
\theta =
\begin{bmatrix}
\theta_0 \\
\theta_1 \\
\theta_2 \\
\vdots \\
\theta_n
\end{bmatrix}
$$

Then:

$$
h_\theta(x) = \theta^T x
$$

### 🔹 Cost Function

$$
J(\theta) = \frac{1}{2m}\sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2
$$

### 🔹 Gradient Descent (vectorized form)

$$
\theta := \theta - \alpha \cdot \frac{1}{m} X^T (X\theta - y)
$$

Where:

* $X$ = matrix of features (m × n)
* $y$ = vector of outputs (m × 1)
* $X\theta - y$ = vector of residuals

---

## 6. 🌄 Why All Parameters Update Simultaneously?

* If we update $\theta_0$ first and use that new value when updating $\theta_1$, updates become inconsistent.
* Correct way: compute all gradients **using old parameters** → update simultaneously.
* This ensures the step is taken in the **true gradient direction**.

---

## 7. ⚡ Local vs Global Minima

* In linear regression: **Cost function is convex** (parabola/bowl).
* There is **only one global minimum**.
* Gradient descent always converges to it (if $\alpha$ is chosen properly).

---

## 8. 🧩 Practical Issues

* **Feature scaling / normalization**

  * Different scales (e.g., age vs income) can slow down convergence.
  * Normalize features to $[0,1]$ or mean 0, std 1.

* **Learning rate tuning**

  * Small enough to converge, large enough to be efficient.
  * Sometimes need trial & error or adaptive methods.

* **Polynomial regression**

  * Linear regression can be extended by adding powers of features ($x^2, x^3$) for nonlinear patterns.

---

## 9. ✅ Recap (easy to memorize)

* Hypothesis: $h_\theta(x) = \theta^T x$
* Cost function: $J(\theta) = \frac{1}{2m}\sum (h_\theta(x)-y)^2$
* Gradient descent update:

$$
\theta_j := \theta_j - \alpha \frac{1}{m}\sum (h_\theta(x)-y)x_j
$$

* Vectorized update:

$$
\theta := \theta - \alpha \cdot \frac{1}{m} X^T (X\theta - y)
$$

* Convex cost ⇒ guaranteed global minimum.

---